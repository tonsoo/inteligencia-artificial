\documentclass[english]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx,url}
\usepackage{babel}
\usepackage{balance}
\usepackage[hidelinks]{hyperref}
\usepackage{caption}

\title{Vieses nos Algoritmos de Recrutamento com Inteligência Artificial: Uma Resenha}

\author{Alysson Cauã de Oliveira Thoaldo \\
Ciência da Computação - Universidade Tuiuti do Paraná \\
\texttt{alysson.thoaldo@utp.edu.br}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Esse trabalho é uma resenha crítica do artigo do Z. Chen (2023), publicado na revista \textit{Humanities and Social Sciences Communications}. O artigo fala sobre os problemas éticos causados pelos vieses dos algoritmos em sistemas de recrutamento com inteligência artificial. A gente vai ver os pontos principais do artigo, exemplos reais, os impactos na sociedade e ideias pra diminuir essas discriminações. O objetivo é entender como a tecnologia pode influenciar o mercado de trabalho e o que pode ser feito pra usar ela de forma mais justa.
\end{abstract}

\section{Introdução}
Hoje em dia, várias empresas estão usando inteligência artificial (IA) pra ajudar no processo de contratar pessoas. Isso acontece porque a IA pode analisar muitos currículos rápido e escolher quem parece melhor para vaga. Parece ótimo, né? Mas o problema é que esses sistemas aprendem com dados antigos que têm vários preconceitos e desigualdades da sociedade. Então, a IA pode acabar copiando esses preconceitos e até piorando eles, sem ninguém perceber. O artigo do Chen (2023) fala justamente desses riscos e discute como podemos evitar que isso aconteça, pensando sempre na ética, ou seja, no que é certo e justo.

\section{Resumo do Artigo}
No artigo, Chen mostra que os algoritmos de IA são treinados com informações de recrutamentos que aconteceram no passado. Isso é importante porque, se no passado algumas pessoas foram prejudicadas, esses dados já vêm com essas injustiças. Por exemplo, se uma empresa contratava mais homens do que mulheres, o algoritmo pode aprender que homens são mais “indicados” e continuar escolhendo mais homens. O mesmo vale pra raça, idade ou outras características. Chen também fala que esses algoritmos são como uma “caixa-preta”, porque a gente não entende muito bem como eles tomam as decisões. Isso dificulta achar onde estão os problemas e cobrar responsabilidade das empresas que usam essas tecnologias.

\section{Casos Reais de Vieses}
O artigo mostra alguns casos reais que ajudaram a entender esses problemas. Um exemplo bem conhecido é o da Amazon, que tinha um sistema de recrutamento automático. Ele acabava dando notas ruins para currículos que tinham palavras relacionadas a mulheres, tipo “liderança feminina” ou “participação em grupo de mulheres”. Isso porque o sistema aprendeu com dados antigos que tinham mais homens contratados e começou a punir essas referências. Outro exemplo são algumas plataformas de emprego que usam IA pra mostrar candidatos. Como elas analisam quem foi contratado antes, acabam escolhendo pessoas parecidas com os já contratados, excluindo grupos minoritários, como negros e pessoas de outras etnias.

\section{Impactos Éticos e Sociais}
Esses vieses trazem muitos problemas éticos e sociais. Do ponto de vista ético, não é justo que uma pessoa seja excluída só por causa do seu gênero, raça ou outro motivo que não tenha a ver com o talento dela. Isso vai contra princípios básicos de justiça e igualdade. Socialmente, esses sistemas acabam ajudando a manter desigualdades que existem há muito tempo. Quando a IA reforça essas discriminações, ela pode dificultar que minorias tenham acesso a empregos melhores, o que afeta a vida delas, a economia e a sociedade como um todo.

\section{Propostas e Regulamentações}
Pra tentar resolver esses problemas, Chen sugere algumas ideias. Uma delas é fazer auditorias regulares nos algoritmos, ou seja, verificar de tempos em tempos se o sistema está tomando decisões justas. Também é importante usar conjuntos de dados mais equilibrados e que tenham informações de diversos grupos, para evitar que o algoritmo fique “viciado”. Outra proposta é que especialistas em ética participem do desenvolvimento dos sistemas de IA, pra garantir que questões de justiça sejam levadas em conta. Além disso, Chen fala que deveriam existir regras e leis que obriguem as empresas a serem transparentes sobre como seus sistemas funcionam e a responderem pelos resultados que eles geram, pra proteger as pessoas que usam essas plataformas.

\section{Conclusão}
A inteligência artificial pode ajudar muito no mercado de trabalho, deixando processos mais rápidos e eficientes. Mas pra isso acontecer do jeito certo, é preciso pensar muito na ética e na justiça. Se a gente não tomar cuidado, a IA pode acabar excluindo pessoas que já sofrem preconceitos, em vez de ajudar a incluir todo mundo. O futuro dos sistemas de recrutamento com IA depende da gente criar e usar essas tecnologias de forma responsável, respeitando os direitos humanos e valorizando a diversidade. Só assim a tecnologia vai ser uma ferramenta pra fazer o mundo mais justo, e não o contrário.

\begin{thebibliography}{1}
\bibitem{chen2023} Chen, Z. (2023). Algorithmic bias in recruitment and selection: A review and research agenda. \textit{Humanities and Social Sciences Communications}, \href{https://www.nature.com/articles/s41599-023-02079-x}{https://www.nature.com/articles/s41599-023-02079-x}
\end{thebibliography}

\end{document}